from .utils import pprint, remove_dirimport jsonfrom copy import deepcopyimport osimport loggingdef similarity(src1, src2, duplicates):    """    :param src1:    :param src2:    :param duplicates:    duplicates    :return: similarity    """    with open("loc.json") as f:        loc = json.load(f)    new_loc = dict()    for k, v in loc.items():        new_loc[k.replace("/", "_")] = v    loc = new_loc    src1 = src1.replace("/home/z1xuan/Desktop/repo/bitcoin/", "")    src2 = src2.replace("/home/z1xuan/Desktop/repo/bitcoin/", "")    src1 = src1.replace("/home/z1xuan/Desktop/repo/ethereum/", "")    src2 = src2.replace("/home/z1xuan/Desktop/repo/ethereum/", "")    total_line = loc[src1] + loc[src2]    dupl_line = 0    for entry in duplicates:        for k, v in duplicates[entry].items():            if isinstance(v, list):                dupl_line += v[0] + v[1]            else:                dupl_line += v*2    return dupl_line/total_linedef jscpd_report(report_loc, src1, src2):    with open(report_loc) as f:        data = json.load(f)    src1_files = dict()    src2_files = dict()    for k, v in data["statistics"]["formats"].items():        for file_name, attrs in v["sources"].items():            if file_name.find(src1) != -1:                src1_files[file_name] = attrs["lines"]            elif file_name.find(src2) != -1:                src2_files[file_name] = attrs["lines"]    duplicates = list()    new_dupl = dict()    for entry in data["duplicates"]:        first_file = entry["firstFile"]["name"]        second_file = entry["secondFile"]["name"]        lines = entry["lines"]        if first_file in src1_files and second_file in src2_files:            duplicates.append([first_file, second_file, lines])        elif first_file in src2_files and second_file in src1_files:            duplicates.append([second_file, first_file, lines])        if first_file in src1_files and second_file in src2_files:            if first_file in new_dupl:                if second_file not in new_dupl[first_file]:                    new_dupl[first_file][second_file] = 0                new_dupl[first_file][second_file] += lines            else:                new_dupl[first_file] = {second_file: lines}        elif first_file in src2_files and second_file in src1_files:            if second_file in new_dupl:                if first_file not in new_dupl[second_file]:                    new_dupl[second_file][first_file] = 0                new_dupl[second_file][first_file] = lines            else:                new_dupl[second_file] = {first_file: lines}    return [src1_files, src2_files, duplicates, new_dupl]def dupl_report(report_loc, src1, src2):    with open(report_loc) as f:        data = json.load(f)    pprint(data)    duplicates = list()    new_dupl = dict()    for entry in data:        if entry["first_file"][:3] == "src1" and entry["second_file"][:3] == "src2":            first_file = src1 + entry["first_file"][4:]            second_file = src2 + entry["second_file"][4:]            first_lines = int(entry["first_lines"][1]) - int(entry["first_lines"][0])            second_lines = int(entry["second_lines"][1]) - int(entry["second_lines"][0])            duplicates.append([first_file, second_file, first_lines, second_lines])        elif entry["first_file"][:3] == "src2" and entry["second_file"][:3] == "src1":            first_file = src1 + entry["second_file"][4:]            second_file = src2 + entry["first_file"][4:]            second_lines = int(entry["first_lines"][1]) - int(entry["first_lines"][0])            first_lines = int(entry["second_lines"][1]) - int(entry["second_lines"][0])            duplicates.append([first_file, second_file, first_lines, second_lines])    return duplicatesdef merge(arr):    arr.sort(key=lambda x: x[0])    m = []    s = -10000    max = -100000    for i in range(len(arr)):        a = arr[i]        if a[0] > max:            if i != 0:                m.append([s, max])            max = a[1]            s = a[0]        else:            if a[1] >= max:                max = a[1]    if max != -100000 and [s, max] not in m:        m.append([s, max])    total_len = 0    for i in range(len(m)):        total_len += m[i][1] - m[i][0]    return total_lendef deckard_report(report_loc, src1, src2):    with open(report_loc) as f:        data = json.load(f)    duplicates = list()    sub = dict()    for k in data:        sub.clear()        sub['src1'] = dict()        sub['src2'] = dict()        for v in k:            file_name = v[3]            first = int(v[4].split(":")[1])            second = int(v[4].split(":")[2])            lines = [first, first + second]            if src1 in file_name:                if file_name not in sub['src1']:                    sub['src1'][file_name] = list()                sub['src1'][file_name].append(lines)            elif src2 in file_name:                if file_name not in sub['src2']:                    sub['src2'][file_name] = list()                sub['src2'][file_name].append(lines)        duplicates.append(deepcopy(sub))    new_dup = dict()    for entry in duplicates:        if len(entry['src1']) == 0 or len(entry['src2']) == 0:            continue        for src1 in entry['src1']:            for src2 in entry['src2']:                if src1 not in new_dup:                    new_dup[src1] = dict()                if src2 not in new_dup[src1]:                    new_dup[src1][src2] = [0, 0]                new_dup[src1][src2][0] += merge(entry['src1'][src1])                new_dup[src1][src2][1] += merge(entry['src2'][src2])    return [duplicates, new_dup]def analyze(report_loc, src1, src2):    dir_name = 'clone_report/' + report_loc.strip().split('/')[-2]    os.makedirs(dir_name)    class_ = report_loc.strip().split('/')[-1]    if class_ == "jscpd-report.json":        res = jscpd_report(report_loc, src1, src2)        if len(res[2]) == 0:            remove_dir(dir_name)            logging.info(report_loc)        else:            file_name = dir_name + "/jscpd.json"            with open(file_name, "w+") as f:                json.dump(res, f)            logging.info(report_loc + " " + os.path.abspath(file_name))    elif class_ == "dupl-report.json":        res = dupl_report(report_loc, src1, src2)        if len(res) == 0:            remove_dir(dir_name)            logging.info(report_loc)        else:            file_name = dir_name + "/dupl.json"            with open(file_name, "w+") as f:                json.dump(res, f)            logging.info(report_loc + " " + os.path.abspath(file_name))    elif class_ == "deckard-report.json":        res = deckard_report(report_loc, src1, src2)        if len(res[0]) == 0:            remove_dir(dir_name)            logging.info(report_loc)        else:            file_name = dir_name + "/deckard.json"            with open(file_name, "w+") as f:                json.dump(res, f)            logging.info(report_loc + " " + os.path.abspath(file_name))    else:        remove_dir(dir_name)log_name = "clone_report/res.log"logging.basicConfig(filename=log_name, filemode='a+', format='%(asctime)s %(levelname)s: %(message)s', datefmt='%y-%b-%d %H:%M:%S', level=logging.INFO)with open("top_100_btc_res.json") as f:    data = json.load(f)for entry in data:    for k,v in data[entry].items():        src1 = entry        src2 = k        report_loc = v        class_ = report_loc.strip().split('/')[-1]        if class_ == "deckard-report.json":            analyze(report_loc, src1, src2)exit()res = dict()with open("files/bitcoin/res.log") as f:    for line in f.readlines():        data = line.split()        if len(data) <=4:            continue        res[data[3]] = data[4]with open("files/ethereum/res.log") as f:    for line in f.readlines():        data = line.split()        if len(data) <=4:            continue        res[data[3]] = data[4]with open("top_100_btc_res.json") as f:    data = json.load(f)    top_100_btc_res = dict()    for entry in data:        for k, v in data[entry].items():            if v in res:                top_100_btc_res[res[v].replace("/home/z1xuan/Desktop/clone_detector/clone_report/", "/Users/zachary/PycharmProjects/code_clone/files/bitcoin/")] = [entry, k]    # pprint(top_100_btc_res)with open("top_100_eth_res.json") as f:    data = json.load(f)    top_100_eth_res = dict()    for entry in data:        for k, v in data[entry].items():            if v in res:                top_100_eth_res[res[v].replace("/home/z1xuan/Desktop/clone_detector/clone_report/", "/Users/zachary/PycharmProjects/code_clone/files/ethereum/")] = [entry, k]for k, v in top_100_btc_res.items():    with open(k) as f:        duplicates = json.load(f)[-1]    if similarity(v[0], v[1], duplicates) > 1:        print(k)# with open("log/done.json") as f:#     data = json.load(f)#     for entry in data:#         for k, v in data[entry].items():#             if v == "/home/z1xuan/Desktop/clone_detector/out/bfBjNYagJgiBYeOlO7mAdqbbJBbuGo/dupl-report.json":#                 print(entry)#                 print(k)# log_name = "clone_report/res.log"# logging.basicConfig(filename=log_name, filemode='a+', format='%(asctime)s %(levelname)s: %(message)s', datefmt='%y-%b-%d %H:%M:%S', level=logging.INFO)# with open("top_100_eth_res.json") as f:    # data = json.load(f)# miss = list()# for entry in data:#     for k,v in data[entry].items():#         src1 = entry#         src2 = k#         report_loc = v#         if os.path.isdir(report_loc):#             miss.append([src1, src2])#             print(src1, src2)#             continue### print(len(miss))# with open("eth_miss.json", "w+") as f:#     json.dump(miss, f)# for entry in data:#     for k,v in data[entry].items():#         src1 = entry#         src2 = k#         report_loc = v#         if os.path.isdir(report_loc):#             miss.append([src1, src2])#             print(src1, src2)#             continue##         class_ = report_loc.strip().split('/')[-1]#         analyze(report_loc, src1, src2)# with open("eth_miss.json", "w+") as f:#     json.dump(miss, f)# pprint(dupl_report("/media/z1xuan/Elements/clone_detector_out/ethereum/uncompressed/out/zfKhbYYOE3kwlqW2bEfdxngVCWJetJ/dupl-report.json", "/home/z1xuan/Desktop/repo/ethereum/sammy007_open-ethereum-pool", "/home/z1xuan/Desktop/repo/ethereum/miguelmota_ethereum-development-with-go-book"))